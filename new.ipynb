{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "scenes = data[\"SCENE\"]\n",
    "labels = data[\"Sarcasm\"].values\n",
    "\n",
    "# Step 1: Find the maximum duration across all audio files\n",
    "def get_max_duration(scenes, sr=22050):\n",
    "    max_duration = 0\n",
    "    for file_path in scenes:\n",
    "        n_f=\"audio_utterance/\"+file_path+\"_u.wav\"\n",
    "        y, _ = librosa.load(n_f, sr=sr)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        if duration > max_duration:\n",
    "            max_duration = duration\n",
    "        #print(\"done with\",n_f)\n",
    "    return max_duration\n",
    "\n",
    "# Calculate the maximum duration (in seconds) across all files\n",
    "max_duration = get_max_duration(scenes)\n",
    "print(f\"Maximum audio duration: {max_duration} seconds\")\n",
    "\n",
    "# Step 2: Convert audio to fixed-size mel spectrograms based on max duration\n",
    "def audio_to_mel_spectrogram(file_path, n_mels=126, sr=22050, duration=max_duration):\n",
    "    n_f=\"audio_utterance/\"+file_path+\"_u.wav\"\n",
    "    y, sr = librosa.load(n_f, sr=sr)\n",
    "    target_length = int(duration * sr)\n",
    "\n",
    "    # Pad or truncate the audio to matcjh the maximum duration\n",
    "    if len(y) > target_length:\n",
    "        y = y[:target_length]\n",
    "    elif len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "\n",
    "    # Convert to mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    #print(\"done wh\",n_f)\n",
    "    return mel_spec_db\n",
    "\n",
    "# Step 3: Prepare the dataset\n",
    "mel_spectrograms = []\n",
    "for scene in scenes:\n",
    "    mel_spec = audio_to_mel_spectrogram(scene)\n",
    "    mel_spectrograms.append(mel_spec)\n",
    "print(\"here\")\n",
    "# Resize spectrograms to ensure consistent input shape (e.g., width of 216)\n",
    "max_frames = max([mel.shape[1] for mel in mel_spectrograms])\n",
    "mel_spectrograms = [librosa.util.fix_length(mel, size=max_frames, axis=1) for mel in mel_spectrograms]\n",
    "X = np.array(mel_spectrograms)\n",
    "X = X[..., np.newaxis]  # Add channel dimension for CNN\n",
    "y = np.array(labels)\n",
    "print(\"split\")\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "np.save(\"x_train_sar.npy\",X_train)\n",
    "np.save(\"x_val_sar.npy\",X_val)\n",
    "np.save(\"y_train.npy\",y_train)\n",
    "np.save(\"y_val.npy\",y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "scenes = data[\"SCENE\"]\n",
    "labels = data[\"Sarcasm\"].values\n",
    "\n",
    "# Step 1: Find the maximum duration across all audio files\n",
    "def get_max_duration(scenes, sr=22050):\n",
    "    max_duration = 0\n",
    "    for file_path in scenes:\n",
    "        n_f=\"audio_context/\"+file_path+\"_c.wav\"\n",
    "        y, _ = librosa.load(n_f, sr=sr)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        if duration > max_duration:\n",
    "            max_duration = duration\n",
    "        #print(\"done with\",n_f)\n",
    "    return max_duration\n",
    "\n",
    "# Calculate the maximum duration (in seconds) across all files\n",
    "max_duration = get_max_duration(scenes)\n",
    "print(f\"Maximum audio duration: {max_duration} seconds\")\n",
    "\n",
    "# Step 2: Convert audio to fixed-size mel spectrograms based on max duration\n",
    "def audio_to_mel_spectrogram(file_path, n_mels=126, sr=22050, duration=max_duration):\n",
    "    n_f=\"audio_utterance/\"+file_path+\"_u.wav\"\n",
    "    y, sr = librosa.load(n_f, sr=sr)\n",
    "    target_length = int(duration * sr)\n",
    "\n",
    "    # Pad or truncate the audio to matcjh the maximum duration\n",
    "    if len(y) > target_length:\n",
    "        y = y[:target_length]\n",
    "    elif len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "\n",
    "    # Convert to mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    #print(\"done wh\",n_f)\n",
    "    return mel_spec_db\n",
    "\n",
    "# Step 3: Prepare the dataset\n",
    "mel_spectrograms = []\n",
    "for scene in scenes:\n",
    "    mel_spec = audio_to_mel_spectrogram(scene)\n",
    "    mel_spectrograms.append(mel_spec)\n",
    "print(\"here\")\n",
    "# Resize spectrograms to ensure consistent input shape (e.g., width of 216)\n",
    "max_frames_1 = max([mel.shape[1] for mel in mel_spectrograms])\n",
    "mel_spectrograms = [librosa.util.fix_length(mel, size=max_frames_1, axis=1) for mel in mel_spectrograms]\n",
    "X = np.array(mel_spectrograms)\n",
    "X = X[..., np.newaxis]  # Add channel dimension for CNN\n",
    "y = np.array(labels)\n",
    "print(\"split\")\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "np.save(\"x_train_con.npy\",X_train)\n",
    "np.save(\"x_val_con.npy\",X_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "X_context_train =  np.load(\"x_train_con.npy\")\n",
    "X_context_val =  np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train = np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val =  np.load(\"x_val_sar.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "\n",
    "# Step 2: Reshape data for ANN\n",
    "# Flattening is handled in the model, but input must have the correct shape.\n",
    "# Ensure input shape matches (128, 216, 1) for ANN branches.\n",
    "X_context_train = X_context_train.reshape(-1, 126, max_frames_1, 1)\n",
    "X_context_val = X_context_val.reshape(-1, 126, max_frames_1, 1)\n",
    "X_sarcasm_train = X_sarcasm_train.reshape(-1, 126, max_frames, 1)\n",
    "X_sarcasm_val = X_sarcasm_val.reshape(-1, 126, max_frames, 1)\n",
    "\n",
    "# Step 3: Build the multimodal ANN model\n",
    "context_input = layers.Input(shape=(126, max_frames_1, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, max_frames, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Flatten the inputs for ANN\n",
    "context_branch = layers.Flatten()(context_input)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_input)\n",
    "\n",
    "# Fully connected layers for context branch\n",
    "context_branch = layers.Dense(128, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(64, activation=\"relu\")(context_branch)\n",
    "\n",
    "# Fully connected layers for sarcasm branch\n",
    "sarcasm_branch = layers.Dense(128, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(64, activation=\"relu\")(sarcasm_branch)\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "# Optional dropout to prevent overfitting\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 6: Generate predictions and calculate metrics\n",
    "y_pred = model.predict([X_context_val, X_sarcasm_val])  # Predict probabilities\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary class labels (0 or 1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_val, y_pred_classes)\n",
    "recall = recall_score(y_val, y_pred_classes)\n",
    "f1 = f1_score(y_val, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "X_context_train =  np.load(\"x_train_con.npy\")\n",
    "X_context_val =  np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train = np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val =  np.load(\"x_val_sar.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "\n",
    "# Step 2: Reshape data for ANN\n",
    "# Flattening is handled in the model, but input must have the correct shape.\n",
    "# Ensure input shape matches (128, 216, 1) for ANN branches.\n",
    "X_context_train = X_context_train.reshape(-1, 126, max_frames_1, 1)\n",
    "X_context_val = X_context_val.reshape(-1, 126, max_frames_1, 1)\n",
    "X_sarcasm_train = X_sarcasm_train.reshape(-1, 126, max_frames, 1)\n",
    "X_sarcasm_val = X_sarcasm_val.reshape(-1, 126, max_frames, 1)\n",
    "\n",
    "# Step 3: Build the multimodal ANN model\n",
    "context_input = layers.Input(shape=(126, max_frames_1, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, max_frames, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Flatten the inputs for ANN\n",
    "context_branch = layers.Flatten()(context_input)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_input)\n",
    "\n",
    "# Fully connected layers for context branch\n",
    "context_branch = layers.Dense(128, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(64, activation=\"relu\")(context_branch)\n",
    "\n",
    "# Fully connected layers for sarcasm branch\n",
    "sarcasm_branch = layers.Dense(128, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(64, activation=\"relu\")(sarcasm_branch)\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "# Optional dropout to prevent overfitting\n",
    "#x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 6: Generate predictions and calculate metrics\n",
    "y_pred = model.predict([X_context_val, X_sarcasm_val])  # Predict probabilities\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary class labels (0 or 1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_val, y_pred_classes)\n",
    "recall = recall_score(y_val, y_pred_classes)\n",
    "f1 = f1_score(y_val, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "X_context_train =  np.load(\"x_train_con.npy\")\n",
    "X_context_val =  np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train = np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val =  np.load(\"x_val_sar.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "\n",
    "# Step 2: Reshape data for ANN\n",
    "# Flattening is handled in the model, but input must have the correct shape.\n",
    "# Ensure input shape matches (128, 216, 1) for ANN branches.\n",
    "X_context_train = X_context_train.reshape(-1, 126, max_frames_1, 1)\n",
    "X_context_val = X_context_val.reshape(-1, 126, max_frames_1, 1)\n",
    "X_sarcasm_train = X_sarcasm_train.reshape(-1, 126, max_frames, 1)\n",
    "X_sarcasm_val = X_sarcasm_val.reshape(-1, 126, max_frames, 1)\n",
    "\n",
    "# Step 3: Build the multimodal ANN model\n",
    "context_input = layers.Input(shape=(126, max_frames_1, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, max_frames, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Flatten the inputs for ANN\n",
    "context_branch = layers.Flatten()(context_input)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_input)\n",
    "\n",
    "# Fully connected layers for context branch\n",
    "context_branch = layers.Dense(128, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(64, activation=\"relu\")(context_branch)\n",
    "\n",
    "# Fully connected layers for sarcasm branch\n",
    "sarcasm_branch = layers.Dense(128, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(64, activation=\"relu\")(sarcasm_branch)\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "# Optional dropout to prevent overfitting\n",
    "#x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 6: Generate predictions and calculate metrics\n",
    "y_pred = model.predict([X_context_val, X_sarcasm_val])  # Predict probabilities\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary class labels (0 or 1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_val, y_pred_classes)\n",
    "recall = recall_score(y_val, y_pred_classes)\n",
    "f1 = f1_score(y_val, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "X_context_train =  np.load(\"x_train_con.npy\")\n",
    "X_context_val =  np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train = np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val =  np.load(\"x_val_sar.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "\n",
    "# Step 2: Reshape data for ANN\n",
    "# Flattening is handled in the model, but input must have the correct shape.\n",
    "# Ensure input shape matches (128, 216, 1) for ANN branches.\n",
    "X_context_train = X_context_train.reshape(-1, 126, max_frames_1, 1)\n",
    "X_context_val = X_context_val.reshape(-1, 126, max_frames_1, 1)\n",
    "X_sarcasm_train = X_sarcasm_train.reshape(-1, 126, max_frames, 1)\n",
    "X_sarcasm_val = X_sarcasm_val.reshape(-1, 126, max_frames, 1)\n",
    "\n",
    "# Step 3: Build the multimodal ANN model\n",
    "context_input = layers.Input(shape=(126, max_frames_1, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, max_frames, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Flatten the inputs for ANN\n",
    "context_branch = layers.Flatten()(context_input)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_input)\n",
    "\n",
    "# Fully connected layers for context branch\n",
    "context_branch = layers.Dense(128, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(64, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(32, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(8, activation=\"relu\")(context_branch)\n",
    "\n",
    "# Fully connected layers for sarcasm branch\n",
    "sarcasm_branch = layers.Dense(128, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(64, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(32, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(8, activation=\"relu\")(sarcasm_branch)\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "# Optional dropout to prevent overfitting\n",
    "#x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 6: Generate predictions and calculate metrics\n",
    "y_pred = model.predict([X_context_val, X_sarcasm_val])  # Predict probabilities\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary class labels (0 or 1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_val, y_pred_classes)\n",
    "recall = recall_score(y_val, y_pred_classes)\n",
    "f1 = f1_score(y_val, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "X_context_train =  np.load(\"x_train_con.npy\")\n",
    "X_context_val =  np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train = np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val =  np.load(\"x_val_sar.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "\n",
    "# Step 2: Reshape data for ANN\n",
    "# Flattening is handled in the model, but input must have the correct shape.\n",
    "# Ensure input shape matches (128, 216, 1) for ANN branches.\n",
    "X_context_train = X_context_train.reshape(-1, 126, max_frames_1, 1)\n",
    "X_context_val = X_context_val.reshape(-1, 126, max_frames_1, 1)\n",
    "X_sarcasm_train = X_sarcasm_train.reshape(-1, 126, max_frames, 1)\n",
    "X_sarcasm_val = X_sarcasm_val.reshape(-1, 126, max_frames, 1)\n",
    "\n",
    "# Step 3: Build the multimodal ANN model\n",
    "context_input = layers.Input(shape=(126, max_frames_1, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, max_frames, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Flatten the inputs for ANN\n",
    "context_branch = layers.Flatten()(context_input)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_input)\n",
    "\n",
    "# Fully connected layers for context branch\n",
    "\n",
    "context_branch = layers.Dense(64, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(32, activation=\"relu\")(context_branch)\n",
    "\n",
    "\n",
    "# Fully connected layers for sarcasm branch\n",
    "\n",
    "sarcasm_branch = layers.Dense(64, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(32, activation=\"relu\")(sarcasm_branch)\n",
    "\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "x = layers.Dense(32, activation=\"relu\")(combined)\n",
    "x = layers.Dense(8, activation=\"relu\")(combined)\n",
    "# Optional dropout to prevent overfitting\n",
    "#x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 6: Generate predictions and calculate metrics\n",
    "y_pred = model.predict([X_context_val, X_sarcasm_val])  # Predict probabilities\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary class labels (0 or 1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_val, y_pred_classes)\n",
    "recall = recall_score(y_val, y_pred_classes)\n",
    "f1 = f1_score(y_val, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "X_context_train =  np.load(\"x_train_con.npy\")\n",
    "X_context_val =  np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train = np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val =  np.load(\"x_val_sar.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "\n",
    "# Step 2: Reshape data for ANN\n",
    "# Flattening is handled in the model, but input must have the correct shape.\n",
    "# Ensure input shape matches (128, 216, 1) for ANN branches.\n",
    "X_context_train = X_context_train.reshape(-1, 126, max_frames_1, 1)\n",
    "X_context_val = X_context_val.reshape(-1, 126, max_frames_1, 1)\n",
    "X_sarcasm_train = X_sarcasm_train.reshape(-1, 126, max_frames, 1)\n",
    "X_sarcasm_val = X_sarcasm_val.reshape(-1, 126, max_frames, 1)\n",
    "\n",
    "# Step 3: Build the multimodal ANN model\n",
    "context_input = layers.Input(shape=(126, max_frames_1, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, max_frames, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Flatten the inputs for ANN\n",
    "context_branch = layers.Flatten()(context_input)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_input)\n",
    "\n",
    "# Fully connected layers for context branch\n",
    "\n",
    "context_branch = layers.Dense(64, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(32, activation=\"relu\")(context_branch)\n",
    "\n",
    "\n",
    "# Fully connected layers for sarcasm branch\n",
    "\n",
    "sarcasm_branch = layers.Dense(64, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(32, activation=\"relu\")(sarcasm_branch)\n",
    "\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "x = layers.Dense(32, activation=\"relu\")(combined)\n",
    "x = layers.Dense(8, activation=\"relu\")(combined)\n",
    "# Optional dropout to prevent overfitting\n",
    "x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 6: Generate predictions and calculate metrics\n",
    "y_pred = model.predict([X_context_val, X_sarcasm_val])  # Predict probabilities\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary class labels (0 or 1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_val, y_pred_classes)\n",
    "recall = recall_score(y_val, y_pred_classes)\n",
    "f1 = f1_score(y_val, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "X_context_train =  np.load(\"x_train_con.npy\")\n",
    "X_context_val =  np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train = np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val =  np.load(\"x_val_sar.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "\n",
    "# Step 2: Reshape data for ANN\n",
    "# Flattening is handled in the model, but input must have the correct shape.\n",
    "# Ensure input shape matches (128, 216, 1) for ANN branches.\n",
    "X_context_train = X_context_train.reshape(-1, 126, max_frames_1, 1)\n",
    "X_context_val = X_context_val.reshape(-1, 126, max_frames_1, 1)\n",
    "X_sarcasm_train = X_sarcasm_train.reshape(-1, 126, max_frames, 1)\n",
    "X_sarcasm_val = X_sarcasm_val.reshape(-1, 126, max_frames, 1)\n",
    "\n",
    "# Step 3: Build the multimodal ANN model\n",
    "context_input = layers.Input(shape=(126, max_frames_1, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, max_frames, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Flatten the inputs for ANN\n",
    "context_branch = layers.Flatten()(context_input)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_input)\n",
    "\n",
    "# Fully connected layers for context branch\n",
    "context_branch = layers.Dense(128, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(64, activation=\"relu\")(context_branch)\n",
    "\n",
    "# Fully connected layers for sarcasm branch\n",
    "sarcasm_branch = layers.Dense(128, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(64, activation=\"relu\")(sarcasm_branch)\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "# Optional dropout to prevent overfitting\n",
    "x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 6: Generate predictions and calculate metrics\n",
    "y_pred = model.predict([X_context_val, X_sarcasm_val])  # Predict probabilities\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary class labels (0 or 1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_val, y_pred_classes)\n",
    "recall = recall_score(y_val, y_pred_classes)\n",
    "f1 = f1_score(y_val, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "X_context_train =  np.load(\"x_train_con.npy\")\n",
    "X_context_val =  np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train = np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val =  np.load(\"x_val_sar.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "\n",
    "# Step 2: Reshape data for ANN\n",
    "# Flattening is handled in the model, but input must have the correct shape.\n",
    "# Ensure input shape matches (128, 216, 1) for ANN branches.\n",
    "X_context_train = X_context_train.reshape(-1, 126, max_frames_1, 1)\n",
    "X_context_val = X_context_val.reshape(-1, 126, max_frames_1, 1)\n",
    "X_sarcasm_train = X_sarcasm_train.reshape(-1, 126, max_frames, 1)\n",
    "X_sarcasm_val = X_sarcasm_val.reshape(-1, 126, max_frames, 1)\n",
    "\n",
    "# Step 3: Build the multimodal ANN model\n",
    "context_input = layers.Input(shape=(126, max_frames_1, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, max_frames, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Flatten the inputs for ANN\n",
    "context_branch = layers.Flatten()(context_input)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_input)\n",
    "\n",
    "# Fully connected layers for context branch\n",
    "context_branch = layers.Dense(128, activation=\"tanh\")(context_branch)\n",
    "context_branch = layers.Dense(64, activation=\"tanh\")(context_branch)\n",
    "\n",
    "# Fully connected layers for sarcasm branch\n",
    "sarcasm_branch = layers.Dense(128, activation=\"tanh\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(64, activation=\"tanh\")(sarcasm_branch)\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"tanh\")(combined)\n",
    "# Optional dropout to prevent overfitting\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 6: Generate predictions and calculate metrics\n",
    "y_pred = model.predict([X_context_val, X_sarcasm_val])  # Predict probabilities\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary class labels (0 or 1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_val, y_pred_classes)\n",
    "recall = recall_score(y_val, y_pred_classes)\n",
    "f1 = f1_score(y_val, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Load data\n",
    "X_context_train =  np.load(\"x_train_con.npy\")\n",
    "X_context_val =  np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train = np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val =  np.load(\"x_val_sar.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "\n",
    "# Step 2: Reshape data for ANN\n",
    "# Flattening is handled in the model, but input must have the correct shape.\n",
    "# Ensure input shape matches (128, 216, 1) for ANN branches.\n",
    "X_context_train = X_context_train.reshape(-1, 126, max_frames_1, 1)\n",
    "X_context_val = X_context_val.reshape(-1, 126, max_frames_1, 1)\n",
    "X_sarcasm_train = X_sarcasm_train.reshape(-1, 126, max_frames, 1)\n",
    "X_sarcasm_val = X_sarcasm_val.reshape(-1, 126, max_frames, 1)\n",
    "\n",
    "# Step 3: Build the multimodal ANN model\n",
    "context_input = layers.Input(shape=(126, max_frames_1, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, max_frames, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Flatten the inputs for ANN\n",
    "context_branch = layers.Flatten()(context_input)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_input)\n",
    "\n",
    "# Fully connected layers for context branch\n",
    "context_branch = layers.Dense(128, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(64, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(32, activation=\"relu\")(context_branch)\n",
    "context_branch = layers.Dense(8, activation=\"relu\")(context_branch)\n",
    "\n",
    "# Fully connected layers for sarcasm branch\n",
    "sarcasm_branch = layers.Dense(128, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(64, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(32, activation=\"relu\")(sarcasm_branch)\n",
    "sarcasm_branch = layers.Dense(8, activation=\"relu\")(sarcasm_branch)\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "# Optional dropout to prevent overfitting\n",
    "#x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 6: Generate predictions and calculate metrics\n",
    "y_pred = model.predict([X_context_val, X_sarcasm_val])  # Predict probabilities\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary class labels (0 or 1)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_val, y_pred_classes)\n",
    "recall = recall_score(y_val, y_pred_classes)\n",
    "f1 = f1_score(y_val, y_pred_classes)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_context_train=np.load(\"x_train_con.npy\")\n",
    "X_context_val=np.load(\"x_val_con.npy\")\n",
    "X_sarcasm_train=np.load(\"x_train_sar.npy\")\n",
    "X_sarcasm_val=np.load(\"x_val_sar.npy\")\n",
    "y_train=np.load(\"y_train.npy\")\n",
    "y_val=np.load(\"y_val.npy\")\n",
    "\n",
    "\n",
    "# Step 4: Build the multimodal CNN model\n",
    "context_input = layers.Input(shape=(126, 512, 1), name=\"context_input\")\n",
    "sarcasm_input = layers.Input(shape=(126, 512, 1), name=\"sarcasm_input\")\n",
    "\n",
    "# Context branch\n",
    "context_branch = layers.Conv2D(64, (3, 3), activation=\"relu\")(context_input)\n",
    "context_branch = layers.MaxPooling2D((2, 2))(context_branch)\n",
    "#context_branch = layers.Conv2D(64, (3, 3), activation=\"relu\")(context_branch)\n",
    "#context_branch = layers.MaxPooling2D((2, 2))(context_branch)\n",
    "context_branch = layers.Flatten()(context_branch)\n",
    "\n",
    "# Sarcasm branch\n",
    "sarcasm_branch = layers.Conv2D(64, (3, 3), activation=\"relu\")(sarcasm_input)\n",
    "sarcasm_branch = layers.MaxPooling2D((2, 2))(sarcasm_branch)\n",
    "#sarcasm_branch = layers.Conv2D(64, (3, 3), activation=\"relu\")(sarcasm_branch)\n",
    "#sarcasm_branch = layers.MaxPooling2D((2, 2))(sarcasm_branch)\n",
    "sarcasm_branch = layers.Flatten()(sarcasm_branch)\n",
    "\n",
    "# Combine both branches\n",
    "combined = layers.Concatenate()([context_branch, sarcasm_branch])\n",
    "x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "#x = layers.Dropout(0.2)(x)\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = models.Model(inputs=[context_input, sarcasm_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# Step 5: Train the model\n",
    "history = model.fit(\n",
    "    [X_context_train, X_sarcasm_train], y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_context_val, X_sarcasm_val], y_val),\n",
    ")\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_context_val, X_sarcasm_val], y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "#model.save(\"context_sarcasm_model.h5\")\n",
    "print(\"Model saved as 'context_sarcasm_model.h5'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
